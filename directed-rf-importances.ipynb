{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sklearn\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the trained model and the data matrix\n",
    "\n",
    "model = pickle.load(open('model_pkl_file','rb'))\n",
    "dat = pickle.load(open('data_pkl_file','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine number of features, as well as their min and max\n",
    "\n",
    "X = dat['test_x']\n",
    "rf = model['modelobj']\n",
    "feat_max = np.amax(X,axis=0)\n",
    "feat_min = np.amin(X,axis=0)\n",
    "n_feat = len(feat_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate random samples, run the trained model on it, and join\n",
    "\n",
    "random_X = np.array([[random.uniform(feat_min[i],feat_max[i]) for i in range(n_feat)] for _ in range(100000)])\n",
    "random_y = rf.predict_proba(random_X)[:,1]\n",
    "mc_mat = np.c_[random_X,random_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform the Monte Carlo integration\n",
    "\n",
    "risk_score_curves = []\n",
    "\n",
    "for i in range(len(feat_max)):\n",
    "    ssize = (feat_max[i] - feat_min[i])/50.\n",
    "    probs = []\n",
    "    for j in range(50):\n",
    "        sub_mat = mc_mat[ (mc_mat[:,i]>feat_min[i]+j*ssize) & (mc_mat[:,i]<feat_min[i]+(j+1)*ssize) , : ]\n",
    "        probs.append([np.sum(sub_mat[:,-1])/len(sub_mat[:,-1]),np.std(sub_mat[:,-1])])\n",
    "    risk_score_curves.append(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove features that the method doesn't currently work with: categorical variables and features with only one feature value\n",
    "\n",
    "nonanarr = []\n",
    "\n",
    "for i in range(len(feat_max)):\n",
    "    if math.isnan(np.array(risk_score_curves)[i,0,0])==True: \n",
    "        #print(model['feature_importances_names'][i])\n",
    "        #print(feat_min[i],feat_max[i])\n",
    "    if math.isnan(np.array(risk_score_curves)[i,0,0])==False: nonanarr.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform edge detection, print and plot transitions\n",
    "\n",
    "for xx in nonanarr:\n",
    "    curve = np.array(risk_score_curves)[xx,:,:]\n",
    "    vals = curve[:,0]\n",
    "    sigmas = curve[:,1]\n",
    "    subarr = []\n",
    "    subarr.append(vals[0])\n",
    "    transitions = []\n",
    "    \n",
    "    for i in range(49):\n",
    "        mean = np.mean(subarr)\n",
    "        subarr.append(vals[i+1])\n",
    "        std = np.std(subarr)\n",
    "        \n",
    "        #if abs(vals[i+1]-mean) > 2.*std:\n",
    "        if abs(vals[i+1]-mean) > 0.1 * sigmas[i+1]:\n",
    "            print(\"!!!\", i, vals[i+1]-mean)\n",
    "            subarr = []\n",
    "            subarr.append(vals[i+1])\n",
    "            transitions.append([i,vals[i+1]-mean])\n",
    "            \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title(model['feature_importances_names'][xx])\n",
    "    ssize = (feat_max[xx]-feat_min[xx])/50.\n",
    "    feat_range = np.arange(feat_min[xx]+0.5*ssize, feat_max[xx],ssize)\n",
    "    plt.errorbar(feat_range, vals, sigmas)\n",
    "    \n",
    "    for i in range(len(transitions)):\n",
    "        xpos = transitions[i][0]\n",
    "        plt.arrow((feat_range[xpos]+feat_range[xpos+1])/2.,(vals[xpos]+vals[xpos+1])/2.,0,transitions[i][1],head_length=0.002,head_width=ssize/1.2,width=ssize/10.,color='red')\n",
    "    \n",
    "    #plt.savefig(model['feature_importances_names'][xx]+'.png')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The old definition for directed importances\n",
    "\n",
    "master = np.array(risk_score_curves)\n",
    "directed_importances = []\n",
    "\n",
    "for i in range(n_feat):\n",
    "    directed_importances.append(master[i,-1,0]-master[i,0,0])\n",
    "    \n",
    "print(np.c_[model['feature_importances_names'],directed_importances])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
